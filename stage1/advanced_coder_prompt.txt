Make sure to generate full implementation (no TODOs) based on following spec.

Think step by step before generation if needed.


## CLI Mode

ai-coder [options] <command>
  -k, --key <key>            API key (default: $OPENROUTER_API_KEY)
  -u, --url <url>            API URL (default: https://openrouter.ai/api/v1/chat/completions)
  -m, --model <model_name>   LLM model name (default: anthropic/claude-3.5-sonnet)
  -h, --help                 Show this help

Commands:
    ask [file1] [file2] [fileN]             Ask about code (just show LLM response)
    commit [file1] [file2] [fileN]          Create git commit based on given prompt

Prompt needs to be provided in stdin like:


echo "Hello, World in JS" | ai-coder commit hello.js


If no command is given - output help message.



# Implementation Details

COMMON MISTAKES TO AVOID:
- IMPORTANT: don't use `const` for anything. Always use `let`. If you used `const` â€“ don't try to set variable again.
- don't require() external npm packages, only Node native 
- VERY VERY IMPORTANT: ALWAYS escape ` backticks inside of JS template literals. For example: ```hello -> `\`\`\`hello`.
- "node-fetch" package NOT ALLOWED. NOT EVER.

LLM API:
- use OpenRouter API which is compatible with OpenAI
- response format is the same as OpenAI
- Use native https module. 
- Use Authorization: Bearer <API key here>
- steam: false
- Display LLM response when it arrives. Extract code only from final result

Error Handling:
- Log all errors with stack traces
- Show user-friendly messages
- Include request/response details

Debugging:
- Use `debug` npm package to output debugging logs at important parts like I/O

Prompt files:
- Make sure to use prompt files in the same __direname as generated module.

Git:

- Use external `git` process
- Git commit template:

```
<summary generated by LLM in a separate call>

Original prompt:

<prompt>

```

# Bootstrap coder for reference:




